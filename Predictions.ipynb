{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "sG4gdKtHsfqJ",
        "outputId": "2025017a-be35-4c08-aace-a415e767445c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-73d46f396dfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminimum_kernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminimum_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ReLU' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import configparser\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as K\n",
        "from keras.utils.vis_utils import plot_model as plot\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, './lib/')\n",
        "\n",
        "#function to obtain data for training/testing (validation)\n",
        "\n",
        "#Define the neural network\n",
        "def get_unet1(n_ch,patch_height,patch_width):\n",
        "    inputs = Input(shape=(n_ch,patch_height,patch_width))\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same',data_format='channels_first')(inputs)\n",
        "    conv1 = Dropout(0.2)(conv1)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv1)\n",
        "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
        "    #\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_first')(pool1)\n",
        "    conv2 = Dropout(0.2)(conv2)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv2)\n",
        "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
        "    #\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same',data_format='channels_first')(pool2)\n",
        "    conv3 = Dropout(0.2)(conv3)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv3)\n",
        "\n",
        "    up1 = UpSampling2D(size=(2, 2))(conv3)\n",
        "    up1 = concatenate([conv2,up1],axis=1)\n",
        "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_first')(up1)\n",
        "    conv4 = Dropout(0.2)(conv4)\n",
        "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv4)\n",
        "    #\n",
        "    up2 = UpSampling2D(size=(2, 2))(conv4)\n",
        "    up2 = concatenate([conv1,up2], axis=1)\n",
        "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same',data_format='channels_first')(up2)\n",
        "    conv5 = Dropout(0.2)(conv5)\n",
        "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv5)\n",
        "    #\n",
        "    conv6 = Conv2D(2, (1, 1), activation='relu',padding='same',data_format='channels_first')(conv5)\n",
        "    conv6 = core.Reshape((2,patch_height*patch_width))(conv6)\n",
        "    conv6 = core.Permute((2,1))(conv6)\n",
        "    ############\n",
        "    conv7 = core.Activation('softmax')(conv6)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=conv7)\n",
        "\n",
        "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.3, nesterov=False)\n",
        "    model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "def get_unet(minimum_kernel=32, do=0, activation=ReLU, iteration=1):\n",
        "    inputs = Input((None, None, 3))\n",
        "    conv1 = Dropout(do)(activation()(Conv2D(minimum_kernel, (3, 3), padding='same')(inputs)))\n",
        "    conv1 = Dropout(do)(activation()(Conv2D(minimum_kernel, (3, 3), padding='same')(conv1)))\n",
        "    a = conv1\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Dropout(do)(activation()(Conv2D(minimum_kernel * 2, (3, 3), padding='same')(pool1)))\n",
        "    conv2 = Dropout(do)(activation()(Conv2D(minimum_kernel * 2, (3, 3), padding='same')(conv2)))\n",
        "    b = conv2\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Dropout(do)(activation()(Conv2D(minimum_kernel * 4, (3, 3), padding='same')(pool2)))\n",
        "    conv3 = Dropout(do)(activation()(Conv2D(minimum_kernel * 4, (3, 3), padding='same')(conv3)))\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Dropout(do)(activation()(Conv2D(minimum_kernel * 8, (3, 3), padding='same')(pool3)))\n",
        "    conv4 = Dropout(do)(activation()(Conv2D(minimum_kernel * 8, (3, 3), padding='same')(conv4)))\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = Dropout(do)(activation()(Conv2D(minimum_kernel * 16, (3, 3), padding='same')(pool4)))\n",
        "    conv5 = Dropout(do)(activation()(Conv2D(minimum_kernel * 16, (3, 3), padding='same')(conv5)))\n",
        "\n",
        "    up6 = concatenate([Conv2DTranspose(minimum_kernel * 8, (2, 2), strides=(2, 2), padding='same')(conv5), conv4],\n",
        "                      axis=3)\n",
        "    conv6 = Dropout(do)(activation()(Conv2D(minimum_kernel * 8, (3, 3), padding='same')(up6)))\n",
        "    conv6 = Dropout(do)(activation()(Conv2D(minimum_kernel * 8, (3, 3), padding='same')(conv6)))\n",
        "\n",
        "    up7 = concatenate([Conv2DTranspose(minimum_kernel * 4, (2, 2), strides=(2, 2), padding='same')(conv6), conv3],\n",
        "                      axis=3)\n",
        "    conv7 = Dropout(do)(activation()(Conv2D(minimum_kernel * 4, (3, 3), padding='same')(up7)))\n",
        "    conv7 = Dropout(do)(activation()(Conv2D(minimum_kernel * 4, (3, 3), padding='same')(conv7)))\n",
        "\n",
        "    up8 = concatenate([Conv2DTranspose(minimum_kernel * 2, (2, 2), strides=(2, 2), padding='same')(conv7), conv2],\n",
        "                      axis=3)\n",
        "    conv8 = Dropout(do)(activation()(Conv2D(minimum_kernel * 2, (3, 3), padding='same')(up8)))\n",
        "    conv8 = Dropout(do)(activation()(Conv2D(minimum_kernel * 2, (3, 3), padding='same')(conv8)))\n",
        "\n",
        "    up9 = concatenate([Conv2DTranspose(minimum_kernel, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
        "    conv9 = Dropout(do)(activation()(Conv2D(minimum_kernel, (3, 3), padding='same')(up9)))\n",
        "    conv9 = Dropout(do)(activation()(Conv2D(minimum_kernel, (3, 3), padding='same')(conv9)))\n",
        "\n",
        "    pt_conv1a = Conv2D(minimum_kernel, (3, 3), padding='same')\n",
        "    pt_activation1a = activation()\n",
        "    pt_dropout1a = Dropout(do)\n",
        "    pt_conv1b = Conv2D(minimum_kernel, (3, 3), padding='same')\n",
        "    pt_activation1b = activation()\n",
        "    pt_dropout1b = Dropout(do)\n",
        "    pt_pooling1 = MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "    pt_conv2a = Conv2D(minimum_kernel * 2, (3, 3), padding='same')\n",
        "    pt_activation2a = activation()\n",
        "    pt_dropout2a = Dropout(do)\n",
        "    pt_conv2b = Conv2D(minimum_kernel * 2, (3, 3), padding='same')\n",
        "    pt_activation2b = activation()\n",
        "    pt_dropout2b = Dropout(do)\n",
        "    pt_pooling2 = MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "    pt_conv3a = Conv2D(minimum_kernel * 4, (3, 3), padding='same')\n",
        "    pt_activation3a = activation()\n",
        "    pt_dropout3a = Dropout(do)\n",
        "    pt_conv3b = Conv2D(minimum_kernel * 4, (3, 3), padding='same')\n",
        "    pt_activation3b = activation()\n",
        "    pt_dropout3b = Dropout(do)\n",
        "\n",
        "    pt_tranconv8 = Conv2DTranspose(minimum_kernel * 2, (2, 2), strides=(2, 2), padding='same')\n",
        "    pt_conv8a = Conv2D(minimum_kernel * 2, (3, 3), padding='same')\n",
        "    pt_activation8a = activation()\n",
        "    pt_dropout8a = Dropout(do)\n",
        "    pt_conv8b = Conv2D(minimum_kernel * 2, (3, 3), padding='same')\n",
        "    pt_activation8b = activation()\n",
        "    pt_dropout8b = Dropout(do)\n",
        "\n",
        "    pt_tranconv9 = Conv2DTranspose(minimum_kernel, (2, 2), strides=(2, 2), padding='same')\n",
        "    pt_conv9a = Conv2D(minimum_kernel, (3, 3), padding='same')\n",
        "    pt_activation9a = activation()\n",
        "    pt_dropout9a = Dropout(do)\n",
        "    pt_conv9b = Conv2D(minimum_kernel, (3, 3), padding='same')\n",
        "    pt_activation9b = activation()\n",
        "    pt_dropout9b = Dropout(do)\n",
        "\n",
        "    conv9s = [conv9]\n",
        "    outs = []\n",
        "    a_layers = [a]\n",
        "    for iteration_id in range(iteration):\n",
        "        out = Conv2D(1, (1, 1), activation='sigmoid', name=f'out1{iteration_id + 1}')(conv9s[-1])\n",
        "        outs.append(out)\n",
        "\n",
        "        model = Model(inputs=[inputs], outputs=[outs[-1]])\n",
        "        count = 0\n",
        "        for i, layer in enumerate(model.layers):\n",
        "            if not layer.name.startswith('out'):\n",
        "                count += 1\n",
        "\n",
        "        conv1 = pt_dropout1a(pt_activation1a(pt_conv1a(conv9s[-1])))\n",
        "        conv1 = pt_dropout1b(pt_activation1b(pt_conv1b(conv1)))\n",
        "        a_layers.append(conv1)\n",
        "        conv1 = concatenate(a_layers, axis=3)\n",
        "        conv1 = Conv2D(minimum_kernel, (1, 1), padding='same')(conv1)\n",
        "        pool1 = pt_pooling1(conv1)\n",
        "\n",
        "        conv2 = pt_dropout2a(pt_activation2a(pt_conv2a(pool1)))\n",
        "        conv2 = pt_dropout2b(pt_activation2b(pt_conv2b(conv2)))\n",
        "        pool2 = pt_pooling2(conv2)\n",
        "\n",
        "        conv3 = pt_dropout3a(pt_activation3a(pt_conv3a(pool2)))\n",
        "        conv3 = pt_dropout3b(pt_activation3b(pt_conv3b(conv3)))\n",
        "\n",
        "        up8 = concatenate([pt_tranconv8(conv3), conv2], axis=3)\n",
        "        conv8 = pt_dropout8a(pt_activation8a(pt_conv8a(up8)))\n",
        "        conv8 = pt_dropout8b(pt_activation8b(pt_conv8b(conv8)))\n",
        "\n",
        "        up9 = concatenate([pt_tranconv9(conv8), conv1], axis=3)\n",
        "        conv9 = pt_dropout9a(pt_activation9a(pt_conv9a(up9)))\n",
        "        conv9 = pt_dropout9b(pt_activation9b(pt_conv9b(conv9)))\n",
        "\n",
        "        conv9s.append(conv9)\n",
        "\n",
        "    out2 = Conv2D(1, (1, 1), activation='sigmoid', name='final_out')(conv9)\n",
        "    outs.append(out2)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=outs)\n",
        "    count = 0\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if not layer.name.startswith('model1_') and not layer.name.startswith('out'):\n",
        "            layer.name = layer.name[:layer.name.rfind('_')]\n",
        "            layer.name = f'model2_{layer.name}_{count}'\n",
        "            count += 1\n",
        "\n",
        "    loss_funcs = {}\n",
        "    for iteration_id in range(iteration):\n",
        "        loss_funcs.update({f'out1{iteration_id + 1}': losses.binary_crossentropy})\n",
        "    loss_funcs.update({'final_out': losses.binary_crossentropy})\n",
        "\n",
        "    metrics = {\n",
        "        \"final_out\": ['accuracy']\n",
        "    }\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=1e-3), loss=loss_funcs, metrics=metrics)\n",
        "\n",
        "    return model\n",
        "\n",
        "#Define the neural network gnet\n",
        "#you need change function call \"get_unet\" to \"get_gnet\" in line 166 before use this network\n",
        "def get_gnet(n_ch,patch_height,patch_width):\n",
        "    inputs = Input((n_ch, patch_height, patch_width))\n",
        "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
        "    conv1 = Dropout(0.2)(conv1)\n",
        "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)\n",
        "    up1 = UpSampling2D(size=(2, 2))(conv1)\n",
        "    #\n",
        "    conv2 = Convolution2D(16, 3, 3, activation='relu', border_mode='same')(up1)\n",
        "    conv2 = Dropout(0.2)(conv2)\n",
        "    conv2 = Convolution2D(16, 3, 3, activation='relu', border_mode='same')(conv2)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    #\n",
        "    conv3 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(pool1)\n",
        "    conv3 = Dropout(0.2)(conv3)\n",
        "    conv3 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv3)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    #\n",
        "    conv4 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(pool2)\n",
        "    conv4 = Dropout(0.2)(conv4)\n",
        "    conv4 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv4)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    #\n",
        "    conv5 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(pool3)\n",
        "    conv5 = Dropout(0.2)(conv5)\n",
        "    conv5 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv5)\n",
        "    #\n",
        "    up2 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n",
        "    conv6 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(up2)\n",
        "    conv6 = Dropout(0.2)(conv6)\n",
        "    conv6 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv6)\n",
        "    #\n",
        "    up3 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
        "    conv7 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(up3)\n",
        "    conv7 = Dropout(0.2)(conv7)\n",
        "    conv7 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv7)\n",
        "    #\n",
        "    up4 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
        "    conv8 = Convolution2D(16, 3, 3, activation='relu', border_mode='same')(up4)\n",
        "    conv8 = Dropout(0.2)(conv8)\n",
        "    conv8 = Convolution2D(16, 3, 3, activation='relu', border_mode='same')(conv8)\n",
        "    #\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv8)\n",
        "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(pool4)\n",
        "    conv9 = Dropout(0.2)(conv9)\n",
        "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv9)\n",
        "    #\n",
        "    conv10 = Convolution2D(2, 1, 1, activation='relu', border_mode='same')(conv9)\n",
        "    conv10 = core.Reshape((2,patch_height*patch_width))(conv10)\n",
        "    conv10 = core.Permute((2,1))(conv10)\n",
        "    ############\n",
        "    conv10 = core.Activation('softmax')(conv10)\n",
        "\n",
        "    model = Model(input=inputs, output=conv10)\n",
        "\n",
        "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.3, nesterov=False)\n",
        "    model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "#========= Load settings from Config file\n",
        "config = configparser.RawConfigParser()\n",
        "config.read('configuration.txt')\n",
        "#patch to the datasets\n",
        "path_data = config.get('data paths', 'path_local')\n",
        "#Experiment name\n",
        "name_experiment = config.get('experiment name', 'name')\n",
        "#training settings\n",
        "N_epochs = int(config.get('training settings', 'N_epochs'))\n",
        "batch_size = int(config.get('training settings', 'batch_size'))\n",
        "\n",
        "#============ Load the data and divided in patches\n",
        "patches_imgs_train, patches_masks_train = get_data_training(\n",
        "    DRIVE_train_imgs_original = path_data + config.get('data paths', 'train_imgs_original'),\n",
        "    DRIVE_train_groudTruth = path_data + config.get('data paths', 'train_groundTruth'),  #masks\n",
        "    patch_height = int(config.get('data attributes', 'patch_height')),\n",
        "    patch_width = int(config.get('data attributes', 'patch_width')),\n",
        "    N_subimgs = int(config.get('training settings', 'N_subimgs')),\n",
        "    inside_FOV = config.getboolean('training settings', 'inside_FOV') #select the patches only inside the FOV  (default == True)\n",
        ")\n",
        "\n",
        "#========= Save a sample of what you're feeding to the neural network ==========\n",
        "N_sample = min(patches_imgs_train.shape[0],40)\n",
        "visualize(group_images(patches_imgs_train[0:N_sample,:,:,:],5),'/content'+'/'+\"sample_input_imgs\")#.show()\n",
        "visualize(group_images(patches_masks_train[0:N_sample,:,:,:],5),'/content'+'/'+\"sample_input_masks\")#.show()\n",
        "\n",
        "\n",
        "#=========== Construct and save the model arcitecture =====\n",
        "n_ch = patches_imgs_train.shape[1]\n",
        "patch_height = patches_imgs_train.shape[2]\n",
        "patch_width = patches_imgs_train.shape[3]\n",
        "#model = get_unet(n_ch, patch_height, patch_width)  #the U-net model\n",
        "model=get_unet()\n",
        "print (\"Check: final output of the network:\")\n",
        "print (model.output_shape)\n",
        "plot(model, to_file='/content'+'/'+name_experiment + '_model.png')   #check how the model looks like\n",
        "json_string = model.to_json()\n",
        "open('/content'+'/'+name_experiment +'_architecture.json', 'w').write(json_string)\n",
        "\n",
        "\n",
        "\n",
        "#============  Training ==================================\n",
        "checkpointer = ModelCheckpoint(filepath='./'+name_experiment+'/'+name_experiment +'_best_weights.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True) #save at each epoch if the validation decreased\n",
        "\n",
        "\n",
        "# def step_decay(epoch):\n",
        "#     lrate = 0.01 #the initial learning rate (by default in keras)\n",
        "#     if epoch==100:\n",
        "#         return 0.005\n",
        "#     else:\n",
        "#         return lrate\n",
        "#\n",
        "# lrate_drop = LearningRateScheduler(step_decay)\n",
        "\n",
        "patches_masks_train = masks_Unet(patches_masks_train)  #reduce memory consumption\n",
        "model.fit(patches_imgs_train, patches_masks_train, epochs=N_epochs, batch_size=batch_size, verbose=2, shuffle=True, validation_split=0.1, callbacks=[checkpointer])\n",
        "\n",
        "\n",
        "#========== Save and test the last model ===================\n",
        "model.save_weights('./'+name_experiment+'/'+name_experiment +'_last_weights.h5', overwrite=True)\n",
        "#test the model\n",
        "# score = model.evaluate(patches_imgs_test, masks_Unet(patches_masks_test), verbose=0)\n",
        "# print('Test score:', score[0])\n",
        "# print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import configparser\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as K\n",
        "from keras.utils.vis_utils import plot_model as plot\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "q6mZCgkgekjZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5KhladM77nGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a543510-c49d-477a-e8c7-511fc0c62a48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mo6225GjszE8"
      },
      "outputs": [],
      "source": [
        "import h5py \n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def load_hdf5(infile):\n",
        "  with h5py.File(infile,\"r\") as f:  #\"with\" close the file after its nested commands\n",
        "    return f[\"image\"][()]\n",
        "\n",
        "def write_hdf5(arr,outfile):\n",
        "  with h5py.File(outfile,\"w\") as f:\n",
        "    f.create_dataset(\"image\", data=arr, dtype=arr.dtype)\n",
        "\n",
        "#convert RGB image in black and white\n",
        "def rgb2gray(rgb):\n",
        "    assert (len(rgb.shape)==4)  #4D arrays\n",
        "    assert (rgb.shape[1]==3)\n",
        "    bn_imgs = rgb[:,0,:,:]*0.299 + rgb[:,1,:,:]*0.587 + rgb[:,2,:,:]*0.114\n",
        "    bn_imgs = np.reshape(bn_imgs,(rgb.shape[0],1,rgb.shape[2],rgb.shape[3]))\n",
        "    return bn_imgs\n",
        "\n",
        "#group a set of images row per columns\n",
        "def group_images(data,per_row):\n",
        "    assert data.shape[0]%per_row==0\n",
        "    assert (data.shape[1]==1 or data.shape[1]==3)\n",
        "    data = np.transpose(data,(0,2,3,1))  #corect format for imshow\n",
        "    all_stripe = []\n",
        "    for i in range(int(data.shape[0]/per_row)):\n",
        "        stripe = data[i*per_row]\n",
        "        for k in range(i*per_row+1, i*per_row+per_row):\n",
        "            stripe = np.concatenate((stripe,data[k]),axis=1)\n",
        "        all_stripe.append(stripe)\n",
        "    totimg = all_stripe[0]\n",
        "    for i in range(1,len(all_stripe)):\n",
        "        totimg = np.concatenate((totimg,all_stripe[i]),axis=0)\n",
        "    return totimg\n",
        "\n",
        "#visualize image (as PIL image, NOT as matplotlib!)\n",
        "def visualize(data,filename):\n",
        "    assert (len(data.shape)==3) #height*width*channels\n",
        "    img = None\n",
        "    if data.shape[2]==1:  #in case it is black and white\n",
        "        data = np.reshape(data,(data.shape[0],data.shape[1]))\n",
        "    if np.max(data)>1:\n",
        "        img = Image.fromarray(data.astype(np.uint8))   #the image is already 0-255\n",
        "    else:\n",
        "        img = Image.fromarray((data*255).astype(np.uint8))  #the image is between 0-1\n",
        "    img.save(filename + '.png')\n",
        "    return img\n",
        "\n",
        "#prepare the mask in the right shape for the Unet\n",
        "def masks_Unet(masks):\n",
        "    assert (len(masks.shape)==4)  #4D arrays\n",
        "    assert (masks.shape[1]==1 )  #check the channel is 1\n",
        "    im_h = masks.shape[2]\n",
        "    im_w = masks.shape[3]\n",
        "    masks = np.reshape(masks,(masks.shape[0],im_h*im_w))\n",
        "    new_masks = np.empty((masks.shape[0],im_h*im_w,2))\n",
        "    for i in range(masks.shape[0]):\n",
        "        for j in range(im_h*im_w):\n",
        "            if  masks[i,j] == 0:\n",
        "                new_masks[i,j,0]=1\n",
        "                new_masks[i,j,1]=0\n",
        "            else:\n",
        "                new_masks[i,j,0]=0\n",
        "                new_masks[i,j,1]=1\n",
        "    return new_masks\n",
        "\n",
        "def pred_to_imgs(pred, patch_height, patch_width, mode=\"original\"):\n",
        "    assert (len(pred.shape)==3)  #3D array: (Npatches,height*width,2)\n",
        "    assert (pred.shape[2]==2 )  #check the classes are 2\n",
        "    pred_images = np.empty((pred.shape[0],pred.shape[1]))  #(Npatches,height*width)\n",
        "    if mode==\"original\":\n",
        "        for i in range(pred.shape[0]):\n",
        "            for pix in range(pred.shape[1]):\n",
        "                pred_images[i,pix]=pred[i,pix,1]\n",
        "    elif mode==\"threshold\":\n",
        "        for i in range(pred.shape[0]):\n",
        "            for pix in range(pred.shape[1]):\n",
        "                if pred[i,pix,1]>=0.5:\n",
        "                    pred_images[i,pix]=1\n",
        "                else:\n",
        "                    pred_images[i,pix]=0\n",
        "    else:\n",
        "        print (\"mode \" +str(mode) +\" not recognized, it can be 'original' or 'threshold'\")\n",
        "        exit()\n",
        "    pred_images = np.reshape(pred_images,(pred_images.shape[0],1, patch_height, patch_width))\n",
        "    return pred_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LA1e8vUos1Ko"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import configparser\n",
        "\n",
        "#To select the same images\n",
        "# random.seed(10)\n",
        "\n",
        "#Load the original data and return the extracted patches for training/testing\n",
        "def get_data_training(DRIVE_train_imgs_original,\n",
        "                      DRIVE_train_groudTruth,\n",
        "                      patch_height,\n",
        "                      patch_width,\n",
        "                      N_subimgs,\n",
        "                      inside_FOV):\n",
        "    train_imgs_original = load_hdf5(DRIVE_train_imgs_original)\n",
        "    train_masks = load_hdf5(DRIVE_train_groudTruth) #masks always the same\n",
        "    # visualize(group_images(train_imgs_original[0:20,:,:,:],5),'imgs_train')#.show()  #check original imgs train\n",
        "\n",
        "\n",
        "    train_imgs = my_PreProc(train_imgs_original)\n",
        "    train_masks = train_masks/255.\n",
        "\n",
        "    train_imgs = train_imgs[:,:,9:574,:]  #cut bottom and top so now it is 565*565\n",
        "    train_masks = train_masks[:,:,9:574,:]  #cut bottom and top so now it is 565*565\n",
        "    data_consistency_check(train_imgs,train_masks)\n",
        "\n",
        "    #check masks are within 0-1\n",
        "    assert(np.min(train_masks)==0 and np.max(train_masks)==1)\n",
        "\n",
        "    print (\"\\ntrain images/masks shape:\")\n",
        "    print (train_imgs.shape)\n",
        "    print (\"train images range (min-max): \" +str(np.min(train_imgs)) +' - '+str(np.max(train_imgs)))\n",
        "    print (\"train masks are within 0-1\\n\")\n",
        "\n",
        "    #extract the TRAINING patches from the full images\n",
        "    patches_imgs_train, patches_masks_train = extract_random(train_imgs,train_masks,patch_height,patch_width,N_subimgs,inside_FOV)\n",
        "    data_consistency_check(patches_imgs_train, patches_masks_train)\n",
        "\n",
        "    print (\"\\ntrain PATCHES images/masks shape:\")\n",
        "    print (patches_imgs_train.shape)\n",
        "    print (\"train PATCHES images range (min-max): \" +str(np.min(patches_imgs_train)) +' - '+str(np.max(patches_imgs_train)))\n",
        "\n",
        "    return patches_imgs_train, patches_masks_train#, patches_imgs_test, patches_masks_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_1HNQ6B7tLIc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "#from help_functions import *\n",
        "\n",
        "#My pre processing (use for both training and testing!)\n",
        "def my_PreProc(data):\n",
        "    assert(len(data.shape)==4)\n",
        "    assert (data.shape[1]==3)  #Use the original images\n",
        "    #black-white conversion\n",
        "    train_imgs = rgb2gray(data)\n",
        "    #my preprocessing:\n",
        "    train_imgs = dataset_normalized(train_imgs)\n",
        "    train_imgs = clahe_equalized(train_imgs)\n",
        "    train_imgs = adjust_gamma(train_imgs, 1.2)\n",
        "    train_imgs = train_imgs/255.  #reduce to 0-1 range\n",
        "    return train_imgs\n",
        "\n",
        "#============================================================\n",
        "#========= PRE PROCESSING FUNCTIONS ========================#\n",
        "#============================================================\n",
        "\n",
        "#==== histogram equalization\n",
        "def histo_equalized(imgs):\n",
        "    assert (len(imgs.shape)==4)  #4D arrays\n",
        "    assert (imgs.shape[1]==1)  #check the channel is 1\n",
        "    imgs_equalized = np.empty(imgs.shape)\n",
        "    for i in range(imgs.shape[0]):\n",
        "        imgs_equalized[i,0] = cv2.equalizeHist(np.array(imgs[i,0], dtype = np.uint8))\n",
        "    return imgs_equalized\n",
        "\n",
        "# CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
        "#adaptive histogram equalization is used. In this, image is divided into small blocks called \"tiles\" (tileSize is 8x8 by default in OpenCV). Then each of these blocks are histogram equalized as usual. So in a small area, histogram would confine to a small region (unless there is noise). If noise is there, it will be amplified. To avoid this, contrast limiting is applied. If any histogram bin is above the specified contrast limit (by default 40 in OpenCV), those pixels are clipped and distributed uniformly to other bins before applying histogram equalization. After equalization, to remove artifacts in tile borders, bilinear interpolation is applied\n",
        "def clahe_equalized(imgs):\n",
        "    assert (len(imgs.shape)==4)  #4D arrays\n",
        "    assert (imgs.shape[1]==1)  #check the channel is 1\n",
        "    #create a CLAHE object (Arguments are optional).\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    imgs_equalized = np.empty(imgs.shape)\n",
        "    for i in range(imgs.shape[0]):\n",
        "        imgs_equalized[i,0] = clahe.apply(np.array(imgs[i,0], dtype = np.uint8))\n",
        "    return imgs_equalized\n",
        "\n",
        "# ===== normalize over the dataset\n",
        "def dataset_normalized(imgs):\n",
        "    assert (len(imgs.shape)==4)  #4D arrays\n",
        "    assert (imgs.shape[1]==1)  #check the channel is 1\n",
        "    imgs_normalized = np.empty(imgs.shape)\n",
        "    imgs_std = np.std(imgs)\n",
        "    imgs_mean = np.mean(imgs)\n",
        "    imgs_normalized = (imgs-imgs_mean)/imgs_std\n",
        "    for i in range(imgs.shape[0]):\n",
        "        imgs_normalized[i] = ((imgs_normalized[i] - np.min(imgs_normalized[i])) / (np.max(imgs_normalized[i])-np.min(imgs_normalized[i])))*255\n",
        "    return imgs_normalized\n",
        "\n",
        "def adjust_gamma(imgs, gamma=1.0):\n",
        "    assert (len(imgs.shape)==4)  #4D arrays\n",
        "    assert (imgs.shape[1]==1)  #check the channel is 1\n",
        "    # build a lookup table mapping the pixel values [0, 255] to\n",
        "    # their adjusted gamma values\n",
        "    invGamma = 1.0 / gamma\n",
        "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
        "    # apply gamma correction using the lookup table\n",
        "    new_imgs = np.empty(imgs.shape)\n",
        "    for i in range(imgs.shape[0]):\n",
        "        new_imgs[i,0] = cv2.LUT(np.array(imgs[i,0], dtype = np.uint8), table)\n",
        "    return new_imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XvQOE9lhthh7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import configparser\n",
        "\n",
        "#To select the same images\n",
        "# random.seed(10)\n",
        "\n",
        "#Load the original data and return the extracted patches for training/testing\n",
        "def get_data_training(DRIVE_train_imgs_original,\n",
        "                      DRIVE_train_groudTruth,\n",
        "                      patch_height,\n",
        "                      patch_width,\n",
        "                      N_subimgs,\n",
        "                      inside_FOV):\n",
        "    train_imgs_original = load_hdf5(DRIVE_train_imgs_original)\n",
        "    train_masks = load_hdf5(DRIVE_train_groudTruth) #masks always the same\n",
        "    # visualize(group_images(train_imgs_original[0:20,:,:,:],5),'imgs_train')#.show()  #check original imgs train\n",
        "\n",
        "    train_imgs = my_PreProc(train_imgs_original)\n",
        "    train_masks = train_masks/255.\n",
        "\n",
        "    train_imgs = train_imgs[:,:,9:574,:]  #cut bottom and top so now it is 565*565\n",
        "    train_masks = train_masks[:,:,9:574,:]  #cut bottom and top so now it is 565*565\n",
        "    data_consistency_check(train_imgs,train_masks)\n",
        "\n",
        "    #check masks are within 0-1\n",
        "    assert(np.min(train_masks)==0 and np.max(train_masks)==1)\n",
        "\n",
        "    #extract the TRAINING patches from the full images\n",
        "    patches_imgs_train, patches_masks_train = extract_random(train_imgs,train_masks,patch_height,patch_width,N_subimgs,inside_FOV)\n",
        "    data_consistency_check(patches_imgs_train, patches_masks_train)\n",
        "\n",
        "    print (\"\\ntrain PATCHES images/masks shape:\")\n",
        "  \n",
        "    print (\"train PATCHES images range (min-max): \" +str(np.min(patches_imgs_train)) +' - '+str(np.max(patches_imgs_train)))\n",
        "\n",
        "    return patches_imgs_train, patches_masks_train#, patches_imgs_test, patches_masks_test\n",
        "\n",
        "#Load the original data and return the extracted patches for training/testing\n",
        "def get_data_testing(DRIVE_test_imgs_original, DRIVE_test_groudTruth, Imgs_to_test, patch_height, patch_width):\n",
        "    ### test\n",
        "    test_imgs_original = load_hdf5(DRIVE_test_imgs_original)\n",
        "    test_masks = load_hdf5(DRIVE_test_groudTruth)\n",
        "\n",
        "    test_imgs = my_PreProc(test_imgs_original)\n",
        "    test_masks = test_masks/255.\n",
        "\n",
        "    #extend both images and masks so they can be divided exactly by the patches dimensions\n",
        "    test_imgs = test_imgs[0:Imgs_to_test,:,:,:]\n",
        "    test_masks = test_masks[0:Imgs_to_test,:,:,:]\n",
        "    test_imgs = paint_border(test_imgs,patch_height,patch_width)\n",
        "    test_masks = paint_border(test_masks,patch_height,patch_width)\n",
        "\n",
        "    data_consistency_check(test_imgs, test_masks)\n",
        "\n",
        "    #check masks are within 0-1\n",
        "    assert(np.max(test_masks)==1  and np.min(test_masks)==0)\n",
        "\n",
        "    print (\"\\ntest images/masks shape:\")\n",
        "    print (test_imgs.shape)\n",
        "    print (\"test images range (min-max): \" +str(np.min(test_imgs)) +' - '+str(np.max(test_imgs)))\n",
        "    print (\"test masks are within 0-1\\n\")\n",
        "\n",
        "    #extract the TEST patches from the full images\n",
        "    patches_imgs_test = extract_ordered(test_imgs,patch_height,patch_width)\n",
        "    patches_masks_test = extract_ordered(test_masks,patch_height,patch_width)\n",
        "    data_consistency_check(patches_imgs_test, patches_masks_test)\n",
        "\n",
        "    print (\"\\ntest PATCHES images/masks shape:\")\n",
        "    print (patches_imgs_test.shape)\n",
        "    print (\"test PATCHES images range (min-max): \" +str(np.min(patches_imgs_test)) +' - '+str(np.max(patches_imgs_test)))\n",
        "\n",
        "    return patches_imgs_test, patches_masks_test\n",
        "\n",
        "# Load the original data and return the extracted patches for testing\n",
        "# return the ground truth in its original shape\n",
        "def get_data_testing_overlap(DRIVE_test_imgs_original, DRIVE_test_groudTruth, Imgs_to_test, patch_height, patch_width, stride_height, stride_width):\n",
        "    ### test\n",
        "    test_imgs_original = load_hdf5(DRIVE_test_imgs_original)\n",
        "    test_masks = load_hdf5(DRIVE_test_groudTruth)\n",
        "\n",
        "    test_imgs = my_PreProc(test_imgs_original)\n",
        "    test_masks = test_masks/255.\n",
        "    #extend both images and masks so they can be divided exactly by the patches dimensions\n",
        "    test_imgs = test_imgs[0:Imgs_to_test,:,:,:]\n",
        "    test_masks = test_masks[0:Imgs_to_test,:,:,:]\n",
        "    test_imgs = paint_border_overlap(test_imgs, patch_height, patch_width, stride_height, stride_width)\n",
        "\n",
        "    #check masks are within 0-1\n",
        "    assert(np.max(test_masks)==1  and np.min(test_masks)==0)\n",
        "    \n",
        "    print (\"test masks are within 0-1\\n\")\n",
        "\n",
        "    #extract the TEST patches from the full images\n",
        "    patches_imgs_test = extract_ordered_overlap(test_imgs,patch_height,patch_width,stride_height,stride_width)\n",
        "    \n",
        "    print (\"test PATCHES images range (min-max): \" +str(np.min(patches_imgs_test)) +' - '+str(np.max(patches_imgs_test)))\n",
        "\n",
        "    return patches_imgs_test, test_imgs.shape[2], test_imgs.shape[3], test_masks\n",
        "\n",
        "#data consinstency check\n",
        "def data_consistency_check(imgs,masks):\n",
        "    assert(len(imgs.shape)==len(masks.shape))\n",
        "    assert(imgs.shape[0]==masks.shape[0])\n",
        "    assert(imgs.shape[2]==masks.shape[2])\n",
        "    assert(imgs.shape[3]==masks.shape[3])\n",
        "    assert(masks.shape[1]==1)\n",
        "    assert(imgs.shape[1]==1 or imgs.shape[1]==3)\n",
        "\n",
        "\n",
        "#extract patches randomly in the full training images\n",
        "#  -- Inside OR in full image\n",
        "def extract_random(full_imgs,full_masks, patch_h,patch_w, N_patches, inside=True):\n",
        "    if (N_patches%full_imgs.shape[0] != 0):\n",
        "        print( \"N_patches: plase enter a multiple of 20\")\n",
        "        exit()\n",
        "    assert (len(full_imgs.shape)==4 and len(full_masks.shape)==4)  #4D arrays\n",
        "    assert (full_imgs.shape[1]==1 or full_imgs.shape[1]==3)  #check the channel is 1 or 3\n",
        "    assert (full_masks.shape[1]==1)   #masks only black and white\n",
        "    assert (full_imgs.shape[2] == full_masks.shape[2] and full_imgs.shape[3] == full_masks.shape[3])\n",
        "    patches = np.empty((N_patches,full_imgs.shape[1],patch_h,patch_w))\n",
        "    patches_masks = np.empty((N_patches,full_masks.shape[1],patch_h,patch_w))\n",
        "    img_h = full_imgs.shape[2]  #height of the full image\n",
        "    img_w = full_imgs.shape[3] #width of the full image\n",
        "    # (0,0) in the center of the image\n",
        "    patch_per_img = int(N_patches/full_imgs.shape[0])  #N_patches equally divided in the full images\n",
        "    print (\"patches per full image: \" +str(patch_per_img))\n",
        "    iter_tot = 0   #iter over the total numbe rof patches (N_patches)\n",
        "    for i in range(full_imgs.shape[0]):  #loop over the full images\n",
        "        k=0\n",
        "        while k <patch_per_img:\n",
        "            x_center = random.randint(0+int(patch_w/2),img_w-int(patch_w/2))\n",
        "            # print \"x_center \" +str(x_center)\n",
        "            y_center = random.randint(0+int(patch_h/2),img_h-int(patch_h/2))\n",
        "            # print \"y_center \" +str(y_center)\n",
        "            #check whether the patch is fully contained in the FOV\n",
        "            if inside==True:\n",
        "                if is_patch_inside_FOV(x_center,y_center,img_w,img_h,patch_h)==False:\n",
        "                    continue\n",
        "            patch = full_imgs[i,:,y_center-int(patch_h/2):y_center+int(patch_h/2),x_center-int(patch_w/2):x_center+int(patch_w/2)]\n",
        "            patch_mask = full_masks[i,:,y_center-int(patch_h/2):y_center+int(patch_h/2),x_center-int(patch_w/2):x_center+int(patch_w/2)]\n",
        "            patches[iter_tot]=patch\n",
        "            patches_masks[iter_tot]=patch_mask\n",
        "            iter_tot +=1   #total\n",
        "            k+=1  #per full_img\n",
        "    return patches, patches_masks\n",
        "\n",
        "#check if the patch is fully contained in the FOV\n",
        "def is_patch_inside_FOV(x,y,img_w,img_h,patch_h):\n",
        "    x_ = x - int(img_w/2) # origin (0,0) shifted to image center\n",
        "    y_ = y - int(img_h/2)  # origin (0,0) shifted to image center\n",
        "    R_inside = 270 - int(patch_h * np.sqrt(2.0) / 2.0) #radius is 270 (from DRIVE db docs), minus the patch diagonal (assumed it is a square #this is the limit to contain the full patch in the FOV\n",
        "    radius = np.sqrt((x_*x_)+(y_*y_))\n",
        "    if radius < R_inside:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "#Divide all the full_imgs in pacthes\n",
        "def extract_ordered(full_imgs, patch_h, patch_w):\n",
        "    assert (len(full_imgs.shape)==4)  #4D arrays\n",
        "    assert (full_imgs.shape[1]==1 or full_imgs.shape[1]==3)  #check the channel is 1 or 3\n",
        "    img_h = full_imgs.shape[2]  #height of the full image\n",
        "    img_w = full_imgs.shape[3] #width of the full image\n",
        "    N_patches_h = int(img_h/patch_h) #round to lowest int\n",
        "    if (img_h%patch_h != 0):\n",
        "        print (\"warning: \" +str(N_patches_h) +\" patches in height, with about \" +str(img_h%patch_h) +\" pixels left over\")\n",
        "    N_patches_w = int(img_w/patch_w) #round to lowest int\n",
        "    if (img_h%patch_h != 0):\n",
        "        print (\"warning: \" +str(N_patches_w) +\" patches in width, with about \" +str(img_w%patch_w) +\" pixels left over\")\n",
        "    print (\"number of patches per image: \" +str(N_patches_h*N_patches_w))\n",
        "    N_patches_tot = (N_patches_h*N_patches_w)*full_imgs.shape[0]\n",
        "    patches = np.empty((N_patches_tot,full_imgs.shape[1],patch_h,patch_w))\n",
        "\n",
        "    iter_tot = 0   #iter over the total number of patches (N_patches)\n",
        "    for i in range(full_imgs.shape[0]):  #loop over the full images\n",
        "        for h in range(N_patches_h):\n",
        "            for w in range(N_patches_w):\n",
        "                patch = full_imgs[i,:,h*patch_h:(h*patch_h)+patch_h,w*patch_w:(w*patch_w)+patch_w]\n",
        "                patches[iter_tot]=patch\n",
        "                iter_tot +=1   #total\n",
        "    assert (iter_tot==N_patches_tot)\n",
        "    return patches  #array with all the full_imgs divided in patches\n",
        "\n",
        "def paint_border_overlap(full_imgs, patch_h, patch_w, stride_h, stride_w):\n",
        "    assert (len(full_imgs.shape)==4)  #4D arrays\n",
        "    assert (full_imgs.shape[1]==1 or full_imgs.shape[1]==3)  #check the channel is 1 or 3\n",
        "    img_h = full_imgs.shape[2]  #height of the full image\n",
        "    img_w = full_imgs.shape[3] #width of the full image\n",
        "    leftover_h = (img_h-patch_h)%stride_h  #leftover on the h dim\n",
        "    leftover_w = (img_w-patch_w)%stride_w  #leftover on the w dim\n",
        "    if (leftover_h != 0):  #change dimension of img_h\n",
        "        print (\"\\nthe side H is not compatible with the selected stride of \" +str(stride_h))\n",
        "        print (\"img_h \" +str(img_h) + \", patch_h \" +str(patch_h) + \", stride_h \" +str(stride_h))\n",
        "        print (\"(img_h - patch_h) MOD stride_h: \" +str(leftover_h))\n",
        "        print (\"So the H dim will be padded with additional \" +str(stride_h - leftover_h) + \" pixels\")\n",
        "        tmp_full_imgs = np.zeros((full_imgs.shape[0],full_imgs.shape[1],img_h+(stride_h-leftover_h),img_w))\n",
        "        tmp_full_imgs[0:full_imgs.shape[0],0:full_imgs.shape[1],0:img_h,0:img_w] = full_imgs\n",
        "        full_imgs = tmp_full_imgs\n",
        "    if (leftover_w != 0):   #change dimension of img_w\n",
        "        print (\"the side W is not compatible with the selected stride of \" +str(stride_w))\n",
        "        print (\"img_w \" +str(img_w) + \", patch_w \" +str(patch_w) + \", stride_w \" +str(stride_w))\n",
        "        print (\"(img_w - patch_w) MOD stride_w: \" +str(leftover_w))\n",
        "        print (\"So the W dim will be padded with additional \" +str(stride_w - leftover_w) + \" pixels\")\n",
        "        tmp_full_imgs = np.zeros((full_imgs.shape[0],full_imgs.shape[1],full_imgs.shape[2],img_w+(stride_w - leftover_w)))\n",
        "        tmp_full_imgs[0:full_imgs.shape[0],0:full_imgs.shape[1],0:full_imgs.shape[2],0:img_w] = full_imgs\n",
        "        full_imgs = tmp_full_imgs\n",
        "    print (\"new full images shape: \\n\" +str(full_imgs.shape))\n",
        "    return full_imgs\n",
        "\n",
        "#Divide all the full_imgs in pacthes\n",
        "def extract_ordered_overlap(full_imgs, patch_h, patch_w,stride_h,stride_w):\n",
        "    assert (len(full_imgs.shape)==4)  #4D arrays\n",
        "    assert (full_imgs.shape[1]==1 or full_imgs.shape[1]==3)  #check the channel is 1 or 3\n",
        "    img_h = full_imgs.shape[2]  #height of the full image\n",
        "    img_w = full_imgs.shape[3] #width of the full image\n",
        "    assert ((img_h-patch_h)%stride_h==0 and (img_w-patch_w)%stride_w==0)\n",
        "    N_patches_img = ((img_h-patch_h)//stride_h+1)*((img_w-patch_w)//stride_w+1)  #// --> division between integers\n",
        "    N_patches_tot = N_patches_img*full_imgs.shape[0]\n",
        "    print (\"Number of patches on h : \" +str(((img_h-patch_h)//stride_h+1)))\n",
        "    print (\"Number of patches on w : \" +str(((img_w-patch_w)//stride_w+1)))\n",
        "    print (\"number of patches per image: \" +str(N_patches_img) +\", totally for this dataset: \" +str(N_patches_tot))\n",
        "    patches = np.empty((N_patches_tot,full_imgs.shape[1],patch_h,patch_w))\n",
        "    iter_tot = 0   #iter over the total number of patches (N_patches)\n",
        "    for i in range(full_imgs.shape[0]):  #loop over the full images\n",
        "        for h in range((img_h-patch_h)//stride_h+1):\n",
        "            for w in range((img_w-patch_w)//stride_w+1):\n",
        "                patch = full_imgs[i,:,h*stride_h:(h*stride_h)+patch_h,w*stride_w:(w*stride_w)+patch_w]\n",
        "                patches[iter_tot]=patch\n",
        "                iter_tot +=1   #total\n",
        "    assert (iter_tot==N_patches_tot)\n",
        "    return patches  #array with all the full_imgs divided in patches\n",
        "\n",
        "def recompone_overlap(preds, img_h, img_w, stride_h, stride_w):\n",
        "    assert (len(preds.shape)==4)  #4D arrays\n",
        "    assert (preds.shape[1]==1 or preds.shape[1]==3)  #check the channel is 1 or 3\n",
        "    patch_h = preds.shape[2]\n",
        "    patch_w = preds.shape[3]\n",
        "    N_patches_h = (img_h-patch_h)//stride_h+1\n",
        "    N_patches_w = (img_w-patch_w)//stride_w+1\n",
        "    N_patches_img = N_patches_h * N_patches_w\n",
        "    print (\"N_patches_h: \" +str(N_patches_h))\n",
        "    print (\"N_patches_w: \" +str(N_patches_w))\n",
        "    print (\"N_patches_img: \" +str(N_patches_img))\n",
        "    assert (preds.shape[0]%N_patches_img==0)\n",
        "    N_full_imgs = preds.shape[0]//N_patches_img\n",
        "    print (\"According to the dimension inserted, there are \" +str(N_full_imgs) +\" full images (of \" +str(img_h)+\"x\" +str(img_w) +\" each)\")\n",
        "    full_prob = np.zeros((N_full_imgs,preds.shape[1],img_h,img_w))  #itialize to zero mega array with sum of Probabilities\n",
        "    full_sum = np.zeros((N_full_imgs,preds.shape[1],img_h,img_w))\n",
        "\n",
        "    k = 0 #iterator over all the patches\n",
        "    for i in range(N_full_imgs):\n",
        "        for h in range((img_h-patch_h)//stride_h+1):\n",
        "            for w in range((img_w-patch_w)//stride_w+1):\n",
        "                full_prob[i,:,h*stride_h:(h*stride_h)+patch_h,w*stride_w:(w*stride_w)+patch_w]+=preds[k]\n",
        "                full_sum[i,:,h*stride_h:(h*stride_h)+patch_h,w*stride_w:(w*stride_w)+patch_w]+=1\n",
        "                k+=1\n",
        "    assert(k==preds.shape[0])\n",
        "    assert(np.min(full_sum)>=1.0)  #at least one\n",
        "    final_avg = full_prob/full_sum\n",
        "    print (final_avg.shape)\n",
        "    assert(np.max(final_avg)<=1.0) #max value for a pixel is 1.0\n",
        "    assert(np.min(final_avg)>=0.0) #min value for a pixel is 0.0\n",
        "    return final_avg\n",
        "\n",
        "#Recompone the full images with the patches\n",
        "def recompone(data,N_h,N_w):\n",
        "    assert (data.shape[1]==1 or data.shape[1]==3)  #check the channel is 1 or 3\n",
        "    assert(len(data.shape)==4)\n",
        "    N_pacth_per_img = N_w*N_h\n",
        "    assert(data.shape[0]%N_pacth_per_img == 0)\n",
        "    N_full_imgs = data.shape[0]/N_pacth_per_img\n",
        "    patch_h = data.shape[2]\n",
        "    patch_w = data.shape[3]\n",
        "    N_pacth_per_img = N_w*N_h\n",
        "    #define and start full recompone\n",
        "    full_recomp = np.empty((N_full_imgs,data.shape[1],N_h*patch_h,N_w*patch_w))\n",
        "    k = 0  #iter full img\n",
        "    s = 0  #iter single patch\n",
        "    while (s<data.shape[0]):\n",
        "        #recompone one:\n",
        "        single_recon = np.empty((data.shape[1],N_h*patch_h,N_w*patch_w))\n",
        "        for h in range(N_h):\n",
        "            for w in range(N_w):\n",
        "                single_recon[:,h*patch_h:(h*patch_h)+patch_h,w*patch_w:(w*patch_w)+patch_w]=data[s]\n",
        "                s+=1\n",
        "        full_recomp[k]=single_recon\n",
        "        k+=1\n",
        "    assert (k==N_full_imgs)\n",
        "    return full_recomp\n",
        "\n",
        "#Extend the full images because patch divison is not exact\n",
        "def paint_border(data,patch_h,patch_w):\n",
        "    assert (len(data.shape)==4)  #4D arrays\n",
        "    assert (data.shape[1]==1 or data.shape[1]==3)  #check the channel is 1 or 3\n",
        "    img_h=data.shape[2]\n",
        "    img_w=data.shape[3]\n",
        "    new_img_h = 0\n",
        "    new_img_w = 0\n",
        "    if (img_h%patch_h)==0:\n",
        "        new_img_h = img_h\n",
        "    else:\n",
        "        new_img_h = ((int(img_h)/int(patch_h))+1)*patch_h\n",
        "    if (img_w%patch_w)==0:\n",
        "        new_img_w = img_w\n",
        "    else:\n",
        "        new_img_w = ((int(img_w)/int(patch_w))+1)*patch_w\n",
        "    new_data = np.zeros((data.shape[0],data.shape[1],new_img_h,new_img_w))\n",
        "    new_data[:,:,0:img_h,0:img_w] = data[:,:,:,:]\n",
        "    return new_data\n",
        "\n",
        "#return only the pixels contained in the FOV, for both images and masks\n",
        "def pred_only_FOV(data_imgs,data_masks,original_imgs_border_masks):\n",
        "    assert (len(data_imgs.shape)==4 and len(data_masks.shape)==4)  #4D arrays\n",
        "    assert (data_imgs.shape[0]==data_masks.shape[0])\n",
        "    assert (data_imgs.shape[2]==data_masks.shape[2])\n",
        "    assert (data_imgs.shape[3]==data_masks.shape[3])\n",
        "    assert (data_imgs.shape[1]==1 and data_masks.shape[1]==1)  #check the channel is 1\n",
        "    height = data_imgs.shape[2]\n",
        "    width = data_imgs.shape[3]\n",
        "    new_pred_imgs = []\n",
        "    new_pred_masks = []\n",
        "    for i in range(data_imgs.shape[0]):  #loop over the full images\n",
        "        for x in range(width):\n",
        "            for y in range(height):\n",
        "                if inside_FOV_DRIVE(i,x,y,original_imgs_border_masks)==True:\n",
        "                    new_pred_imgs.append(data_imgs[i,:,y,x])\n",
        "                    new_pred_masks.append(data_masks[i,:,y,x])\n",
        "    new_pred_imgs = np.asarray(new_pred_imgs)\n",
        "    new_pred_masks = np.asarray(new_pred_masks)\n",
        "    return new_pred_imgs, new_pred_masks\n",
        "\n",
        "#function to set to black everything outside the FOV, in a full image\n",
        "def kill_border(data, original_imgs_border_masks):\n",
        "    assert (len(data.shape)==4)  #4D arrays\n",
        "    assert (data.shape[1]==1 or data.shape[1]==3)  #check the channel is 1 or 3\n",
        "    height = data.shape[2]\n",
        "    width = data.shape[3]\n",
        "    for i in range(data.shape[0]):  #loop over the full images\n",
        "        for x in range(width):\n",
        "            for y in range(height):\n",
        "                if inside_FOV_DRIVE(i,x,y,original_imgs_border_masks)==False:\n",
        "                    data[i,:,y,x]=0.0\n",
        "\n",
        "def inside_FOV_DRIVE(i, x, y, DRIVE_masks):\n",
        "    assert (len(DRIVE_masks.shape)==4)  #4D arrays\n",
        "    assert (DRIVE_masks.shape[1]==1)  #DRIVE masks is black and white\n",
        "    # DRIVE_masks = DRIVE_masks/255.  #NOOO!! otherwise with float numbers takes forever!!\n",
        "\n",
        "    if (x >= DRIVE_masks.shape[3] or y >= DRIVE_masks.shape[2]): #my image bigger than the original\n",
        "        return False\n",
        "\n",
        "    if (DRIVE_masks[i,0,y,x]>0):  #0==black pixels\n",
        "        # print DRIVE_masks[i,0,y,x]  #verify it is working right\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "class DropBlock2D(keras.layers.Layer):\n",
        "    \"\"\"See: https://arxiv.org/pdf/1810.12890.pdf\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 block_size,\n",
        "                 keep_prob,\n",
        "                 sync_channels=False,\n",
        "                 data_format=None,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initialize the layer.\n",
        "        :param block_size: Size for each mask block.\n",
        "        :param keep_prob: Probability of keeping the original feature.\n",
        "        :param sync_channels: Whether to use the same dropout for all channels.\n",
        "        :param data_format: 'channels_first' or 'channels_last' (default).\n",
        "        :param kwargs: Arguments for parent class.\n",
        "        \"\"\"\n",
        "        super(DropBlock2D, self).__init__(**kwargs)\n",
        "        self.block_size = block_size\n",
        "        self.keep_prob = keep_prob\n",
        "        self.sync_channels = sync_channels\n",
        "        self.data_format = K.normalize_data_format(data_format)\n",
        "        self.input_spec = keras.engine.base_layer.InputSpec(ndim=4)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'block_size': self.block_size,\n",
        "                  'keep_prob': self.keep_prob,\n",
        "                  'sync_channels': self.sync_channels,\n",
        "                  'data_format': self.data_format}\n",
        "        base_config = super(DropBlock2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def _get_gamma(self, height, width):\n",
        "        \"\"\"Get the number of activation units to drop\"\"\"\n",
        "        height, width = K.cast(height, K.floatx()), K.cast(width, K.floatx())\n",
        "        block_size = K.constant(self.block_size, dtype=K.floatx())\n",
        "        return ((1.0 - self.keep_prob) / (block_size ** 2)) *\\\n",
        "               (height * width / ((height - block_size + 1.0) * (width - block_size + 1.0)))\n",
        "\n",
        "    def _compute_valid_seed_region(self, height, width):\n",
        "        positions = K.concatenate([\n",
        "            K.expand_dims(K.tile(K.expand_dims(K.arange(height), axis=1), [1, width]), axis=-1),\n",
        "            K.expand_dims(K.tile(K.expand_dims(K.arange(width), axis=0), [height, 1]), axis=-1),\n",
        "        ], axis=-1)\n",
        "        half_block_size = self.block_size // 2\n",
        "        valid_seed_region = K.switch(\n",
        "            K.all(\n",
        "                K.stack(\n",
        "                    [\n",
        "                        positions[:, :, 0] >= half_block_size,\n",
        "                        positions[:, :, 1] >= half_block_size,\n",
        "                        positions[:, :, 0] < height - half_block_size,\n",
        "                        positions[:, :, 1] < width - half_block_size,\n",
        "                    ],\n",
        "                    axis=-1,\n",
        "                ),\n",
        "                axis=-1,\n",
        "            ),\n",
        "            K.ones((height, width)),\n",
        "            K.zeros((height, width)),\n",
        "        )\n",
        "        return K.expand_dims(K.expand_dims(valid_seed_region, axis=0), axis=-1)\n",
        "\n",
        "    def _compute_drop_mask(self, shape):\n",
        "        height, width = shape[1], shape[2]\n",
        "        mask = K.random_binomial(shape, p=self._get_gamma(height, width))\n",
        "        mask *= self._compute_valid_seed_region(height, width)\n",
        "        mask = keras.layers.MaxPool2D(\n",
        "            pool_size=(self.block_size, self.block_size),\n",
        "            padding='same',\n",
        "            strides=1,\n",
        "            data_format='channels_last',\n",
        "        )(mask)\n",
        "        return 1.0 - mask\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "\n",
        "        def dropped_inputs():\n",
        "            outputs = inputs\n",
        "            if self.data_format == 'channels_first':\n",
        "                outputs = K.permute_dimensions(outputs, [0, 2, 3, 1])\n",
        "            shape = K.shape(outputs)\n",
        "            if self.sync_channels:\n",
        "                mask = self._compute_drop_mask([shape[0], shape[1], shape[2], 1])\n",
        "            else:\n",
        "                mask = self._compute_drop_mask(shape)\n",
        "            outputs = outputs * mask *\\\n",
        "                (K.cast(K.prod(shape), dtype=K.floatx()) / K.sum(mask))\n",
        "            if self.data_format == 'channels_first':\n",
        "                outputs = K.permute_dimensions(outputs, [0, 3, 1, 2])\n",
        "            return outputs\n",
        "\n",
        "        return K.in_train_phase(dropped_inputs, inputs, training=training)"
      ],
      "metadata": {
        "id": "hX8tBJJSeMoX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oUFWpKt03Z8S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30005ade-5e79-4187-b476-7f2ac0227723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "the side H is not compatible with the selected stride of 5\n",
            "img_h 584, patch_h 48, stride_h 5\n",
            "(img_h - patch_h) MOD stride_h: 1\n",
            "So the H dim will be padded with additional 4 pixels\n",
            "the side W is not compatible with the selected stride of 5\n",
            "img_w 565, patch_w 48, stride_w 5\n",
            "(img_w - patch_w) MOD stride_w: 2\n",
            "So the W dim will be padded with additional 3 pixels\n",
            "new full images shape: \n",
            "(2, 1, 588, 568)\n",
            "test masks are within 0-1\n",
            "\n",
            "Number of patches on h : 109\n",
            "Number of patches on w : 105\n",
            "number of patches per image: 11445, totally for this dataset: 22890\n",
            "test PATCHES images range (min-max): 0.0 - 1.0\n",
            "716/716 - 17s - 17s/epoch - 24ms/step\n",
            "predicted images size :\n",
            "(22890, 2304, 2)\n",
            "N_patches_h: 109\n",
            "N_patches_w: 105\n",
            "N_patches_img: 11445\n",
            "According to the dimension inserted, there are 2 full images (of 588x568 each)\n",
            "(2, 1, 588, 568)\n",
            "Orig imgs shape: (2, 1, 584, 565)\n",
            "pred imgs shape: (2, 1, 584, 565)\n",
            "Gtruth imgs shape: (2, 1, 584, 565)\n",
            "\n",
            "\n",
            "========  Evaluate the results =======================\n",
            "Calculating results only inside the FOV:\n",
            "y scores pixels: 452747 (radius 270: 270*270*3.14==228906), including background around retina: 659920 (584*565==329960)\n",
            "y true pixels: 452747 (radius 270: 270*270*3.14==228906), including background around retina: 659920 (584*565==329960)\n",
            "\n",
            "Area under the ROC curve: 0.9786061960196056\n",
            "\n",
            "Area under Precision-Recall curve: 0.9146312546917181\n",
            "\n",
            "Confusion matrix:  Custom threshold (for positive) of 0.5\n",
            "[[387997   4287]\n",
            " [ 17153  43310]]\n",
            "Global Accuracy: 0.9526446337579266\n",
            "Specificity: 0.9890716929571433\n",
            "Sensitivity: 0.7163058399351669\n",
            "Precision: 0.9099312981910624\n",
            "\n",
            "Jaccard similarity score: 0.6688803088803089\n",
            "\n",
            "F1 score (F-measure): 0.801591708310198\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import configparser\n",
        "from matplotlib import pyplot as plt\n",
        "#Keras\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Model\n",
        "#scikit learn\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "#from sklearn.metrics import jaccard_similarity_score\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics import f1_score\n",
        "import sys\n",
        "sys.path.insert(0, './lib/')\n",
        "# help_functions.py\n",
        "\n",
        "#========= CONFIG FILE TO READ FROM =======\n",
        "config = configparser.RawConfigParser()\n",
        "config.read('configuration.txt')\n",
        "#===========================================\n",
        "#run the training on invariant or local\n",
        "path_data = config.get('data paths', 'path_local')\n",
        "\n",
        "#original test images (for FOV selection)\n",
        "DRIVE_test_imgs_original = path_data + config.get('data paths', 'test_imgs_original')\n",
        "test_imgs_orig = load_hdf5(DRIVE_test_imgs_original)\n",
        "full_img_height = test_imgs_orig.shape[2]\n",
        "full_img_width = test_imgs_orig.shape[3]\n",
        "#the border masks provided by the DRIVE\n",
        "DRIVE_test_border_masks = path_data + config.get('data paths', 'test_border_masks')\n",
        "test_border_masks = load_hdf5(DRIVE_test_border_masks)\n",
        "# dimension of the patches\n",
        "patch_height = int(config.get('data attributes', 'patch_height'))\n",
        "patch_width = int(config.get('data attributes', 'patch_width'))\n",
        "#the stride in case output with average\n",
        "stride_height = int(config.get('testing settings', 'stride_height'))\n",
        "stride_width = int(config.get('testing settings', 'stride_width'))\n",
        "assert (stride_height < patch_height and stride_width < patch_width)\n",
        "#model name\n",
        "name_experiment = config.get('experiment name', 'name')\n",
        "path_experiment = './' +name_experiment +'/'\n",
        "#N full images to be predicted\n",
        "Imgs_to_test = int(config.get('testing settings', 'full_images_to_test'))\n",
        "#Grouping of the predicted images\n",
        "N_visual = int(config.get('testing settings', 'N_group_visual'))\n",
        "#====== average mode ===========\n",
        "average_mode = config.getboolean('testing settings', 'average_mode')\n",
        "\n",
        "# #ground truth\n",
        "# gtruth= path_data + config.get('data paths', 'test_groundTruth')\n",
        "# img_truth= load_hdf5(gtruth)\n",
        "# visualize(group_images(test_imgs_orig[0:20,:,:,:],5),'original')#.show()\n",
        "# visualize(group_images(test_border_masks[0:20,:,:,:],5),'borders')#.show()\n",
        "# visualize(group_images(img_truth[0:20,:,:,:],5),'gtruth')#.show()\n",
        "\n",
        "#============ Load the data and divide in patches\n",
        "patches_imgs_test = None\n",
        "new_height = None\n",
        "new_width = None\n",
        "masks_test  = None\n",
        "patches_masks_test = None\n",
        "if average_mode == True:\n",
        "    patches_imgs_test, new_height, new_width, masks_test = get_data_testing_overlap(\n",
        "        DRIVE_test_imgs_original = DRIVE_test_imgs_original,  #original\n",
        "        DRIVE_test_groudTruth = path_data + config.get('data paths', 'test_groundTruth'),  #masks\n",
        "        Imgs_to_test = int(config.get('testing settings', 'full_images_to_test')),\n",
        "        patch_height = patch_height,\n",
        "        patch_width = patch_width,\n",
        "        stride_height = stride_height,\n",
        "        stride_width = stride_width\n",
        "    )\n",
        "else:\n",
        "    patches_imgs_test, patches_masks_test = get_data_testing(\n",
        "        DRIVE_test_imgs_original = DRIVE_test_imgs_original,  #original\n",
        "        DRIVE_test_groudTruth = path_data + config.get('data paths', 'test_groundTruth'),  #masks\n",
        "        Imgs_to_test = int(config.get('testing settings', 'full_images_to_test')),\n",
        "        patch_height = patch_height,\n",
        "        patch_width = patch_width,\n",
        "    )\n",
        "\n",
        "#================ Run the prediction of the patches ==================================\n",
        "best_last = config.get('testing settings', 'best_last')\n",
        "#Load the saved model\n",
        "model = model_from_json(open('/content/'+name_experiment +'_architecture.json').read(),custom_objects={'DropBlock2D':DropBlock2D})\n",
        "model.load_weights(path_experiment+name_experiment + '_'+best_last+'_weights.h5')\n",
        "#Calculate the predictions\n",
        "predictions = model.predict(patches_imgs_test, batch_size=32, verbose=2)\n",
        "print (\"predicted images size :\")\n",
        "print (predictions.shape)\n",
        "\n",
        "#===== Convert the prediction arrays in corresponding images\n",
        "pred_patches = pred_to_imgs(predictions, patch_height, patch_width, \"original\")\n",
        "\n",
        "#========== Elaborate and visualize the predicted images ====================\n",
        "pred_imgs = None\n",
        "orig_imgs = None\n",
        "gtruth_masks = None\n",
        "if average_mode == True:\n",
        "    pred_imgs = recompone_overlap(pred_patches, new_height, new_width, stride_height, stride_width)# predictions\n",
        "    orig_imgs = my_PreProc(test_imgs_orig[0:pred_imgs.shape[0],:,:,:])    #originals\n",
        "    gtruth_masks = masks_test  #ground truth masks\n",
        "else:\n",
        "    pred_imgs = recompone(pred_patches,13,12)       # predictions\n",
        "    orig_imgs = recompone(patches_imgs_test,13,12)  # originals\n",
        "    gtruth_masks = recompone(patches_masks_test,13,12)  #masks\n",
        "# apply the DRIVE masks on the repdictions #set everything outside the FOV to zero!!\n",
        "kill_border(pred_imgs, test_border_masks)  #DRIVE MASK  #only for visualization\n",
        "## back to original dimensions\n",
        "orig_imgs = orig_imgs[:,:,0:full_img_height,0:full_img_width]\n",
        "pred_imgs = pred_imgs[:,:,0:full_img_height,0:full_img_width]\n",
        "gtruth_masks = gtruth_masks[:,:,0:full_img_height,0:full_img_width]\n",
        "print (\"Orig imgs shape: \" +str(orig_imgs.shape))\n",
        "print (\"pred imgs shape: \" +str(pred_imgs.shape))\n",
        "print (\"Gtruth imgs shape: \" +str(gtruth_masks.shape))\n",
        "visualize(group_images(orig_imgs,N_visual),path_experiment+\"all_originals\")#.show()\n",
        "visualize(group_images(pred_imgs,N_visual),path_experiment+\"all_predictions\")#.show()\n",
        "visualize(group_images(gtruth_masks,N_visual),path_experiment+\"all_groundTruths\")#.show()\n",
        "#visualize results comparing mask and prediction:\n",
        "assert (orig_imgs.shape[0]==pred_imgs.shape[0] and orig_imgs.shape[0]==gtruth_masks.shape[0])\n",
        "N_predicted = orig_imgs.shape[0]\n",
        "group = N_visual\n",
        "assert (N_predicted%group==0)\n",
        "for i in range(int(N_predicted/group)):\n",
        "    orig_stripe = group_images(orig_imgs[i*group:(i*group)+group,:,:,:],group)\n",
        "    masks_stripe = group_images(gtruth_masks[i*group:(i*group)+group,:,:,:],group)\n",
        "    pred_stripe = group_images(pred_imgs[i*group:(i*group)+group,:,:,:],group)\n",
        "    total_img = np.concatenate((orig_stripe,masks_stripe,pred_stripe),axis=0)\n",
        "    visualize(total_img,path_experiment+name_experiment +\"_Original_GroundTruth_Prediction\"+str(i))#.show()\n",
        "\n",
        "#====== Evaluate the results\n",
        "print (\"\\n\\n========  Evaluate the results =======================\")\n",
        "#predictions only inside the FOV\n",
        "y_scores, y_true = pred_only_FOV(pred_imgs,gtruth_masks, test_border_masks)  #returns data only inside the FOV\n",
        "print (\"Calculating results only inside the FOV:\")\n",
        "print (\"y scores pixels: \" +str(y_scores.shape[0]) +\" (radius 270: 270*270*3.14==228906), including background around retina: \" +str(pred_imgs.shape[0]*pred_imgs.shape[2]*pred_imgs.shape[3]) +\" (584*565==329960)\")\n",
        "print (\"y true pixels: \" +str(y_true.shape[0]) +\" (radius 270: 270*270*3.14==228906), including background around retina: \" +str(gtruth_masks.shape[2]*gtruth_masks.shape[3]*gtruth_masks.shape[0])+\" (584*565==329960)\"\n",
        ")\n",
        "#Area under the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve((y_true), y_scores)\n",
        "AUC_ROC = roc_auc_score(y_true, y_scores)\n",
        "# test_integral = np.trapz(tpr,fpr) #trapz is numpy integration\n",
        "print (\"\\nArea under the ROC curve: \" +str(AUC_ROC))\n",
        "roc_curve =plt.figure()\n",
        "plt.plot(fpr,tpr,'-',label='Area Under the Curve (AUC = %0.4f)' % AUC_ROC)\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel(\"FPR (False Positive Rate)\")\n",
        "plt.ylabel(\"TPR (True Positive Rate)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(path_experiment+\"ROC.png\")\n",
        "\n",
        "#Precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
        "precision = np.fliplr([precision])[0]  #so the array is increasing (you won't get negative AUC)\n",
        "recall = np.fliplr([recall])[0]  #so the array is increasing (you won't get negative AUC)\n",
        "AUC_prec_rec = np.trapz(precision,recall)\n",
        "print (\"\\nArea under Precision-Recall curve: \" +str(AUC_prec_rec))\n",
        "prec_rec_curve = plt.figure()\n",
        "plt.plot(recall,precision,'-',label='Area Under the Curve (AUC = %0.4f)' % AUC_prec_rec)\n",
        "plt.title('Precision - Recall curve')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(path_experiment+\"Precision_recall.png\")\n",
        "\n",
        "#Confusion matrix\n",
        "threshold_confusion = 0.5\n",
        "print (\"\\nConfusion matrix:  Custom threshold (for positive) of \" +str(threshold_confusion))\n",
        "y_pred = np.empty((y_scores.shape[0]))\n",
        "for i in range(y_scores.shape[0]):\n",
        "    if y_scores[i]>=threshold_confusion:\n",
        "        y_pred[i]=1\n",
        "    else:\n",
        "        y_pred[i]=0\n",
        "confusion = confusion_matrix(y_true, y_pred)\n",
        "print (confusion)\n",
        "accuracy = 0\n",
        "if float(np.sum(confusion))!=0:\n",
        "    accuracy = float(confusion[0,0]+confusion[1,1])/float(np.sum(confusion))\n",
        "print (\"Global Accuracy: \" +str(accuracy))\n",
        "specificity = 0\n",
        "if float(confusion[0,0]+confusion[0,1])!=0:\n",
        "    specificity = float(confusion[0,0])/float(confusion[0,0]+confusion[0,1])\n",
        "print (\"Specificity: \" +str(specificity))\n",
        "sensitivity = 0\n",
        "if float(confusion[1,1]+confusion[1,0])!=0:\n",
        "    sensitivity = float(confusion[1,1])/float(confusion[1,1]+confusion[1,0])\n",
        "print (\"Sensitivity: \" +str(sensitivity))\n",
        "precision = 0\n",
        "if float(confusion[1,1]+confusion[0,1])!=0:\n",
        "    precision = float(confusion[1,1])/float(confusion[1,1]+confusion[0,1])\n",
        "print (\"Precision: \" +str(precision))\n",
        "\n",
        "#Jaccard similarity index\n",
        "jaccard_index = jaccard_score(y_true, y_pred)\n",
        "print (\"\\nJaccard similarity score: \" +str(jaccard_index))\n",
        "\n",
        "#F1 score\n",
        "F1_score = f1_score(y_true, y_pred, labels=None, average='binary', sample_weight=None)\n",
        "print (\"\\nF1 score (F-measure): \" +str(F1_score))\n",
        "\n",
        "#Save the results\n",
        "file_perf = open(path_experiment+'performances.txt', 'w')\n",
        "file_perf.write(\"Area under the ROC curve: \"+str(AUC_ROC)\n",
        "                + \"\\nArea under Precision-Recall curve: \" +str(AUC_prec_rec)\n",
        "                + \"\\nJaccard similarity score: \" +str(jaccard_index)\n",
        "                + \"\\nF1 score (F-measure): \" +str(F1_score)\n",
        "                +\"\\n\\nConfusion matrix:\"\n",
        "                +str(confusion)\n",
        "                +\"\\nACCURACY: \" +str(accuracy)\n",
        "                +\"\\nSENSITIVITY: \" +str(sensitivity)\n",
        "                +\"\\nSPECIFICITY: \" +str(specificity)\n",
        "                +\"\\nPRECISION: \" +str(precision)\n",
        "                )\n",
        "file_perf.close()  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "94GYph3UaomX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mr7oLloKaVhn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Training and predictions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}